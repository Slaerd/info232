{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"logo.jpg\", width=300, ALIGN=\"left\">\n",
    "<center>\n",
    "<h1>Mini Projets 2019 (Info 232)</h1>\n",
    "Isabelle Guyon <br>\n",
    "info232@chalearn.org <br>\n",
    "</center>\n",
    "<span style=\"color:red\"> <h1> 2. Apprentissage </h1> </span>\n",
    "\n",
    "<br>This code was tested with <br>\n",
    "Python 3.6.6 |Anaconda custom (64-bit)| (default, Jun 28 2018, 11:07:29) (https://anaconda.org/)<br>\n",
    "<i> Adapted for Chalab by Isabelle Guyon from original code of Balázs Kégl</i> <br>\n",
    "<a href=\"http://www.datascience-paris-saclay.fr\">Paris Saclay Center for Data Science (CDS)</a>\n",
    "</center>\n",
    "    \n",
    "<br> <b>Save your notebook often with menu File + Save and Checkpoint.</b>\n",
    "<br> <b>Before you push your homework to your GitHub repo, use  Kernel + Restart and Run all.</b> \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h2>Introduction </h2>\n",
    "    <p>\n",
    "     <br>\n",
    "Cette semaine, nous continuons a etudier le dataset Iris du <a href=\"http://archive.ics.uci.edu/ml/datasets/Iris\">UCI Machine Learning Repository</a>.\n",
    "        <br> Votre but est de faire une soumission sur la <a href=\"https://codalab.lri.fr/competitions/204\">competition Iris de Codalab</a>.\n",
    "        Votre travail de la semaine derniere vous a permis d'etre embauche a Super Flora, une companie de distribution de fleurs. Votre tache est de controler automatiquement des lots de fleurs en les classant en <b>3 categories (virginica, setosa, et versicolor)</b>, en vous basant sur 4 variables: <b>la longueur et la largeur des petales et des sepales</b>.\n",
    "    <p> Ce Jupyter notebook est tres semblable au <b>\"starting kit\"</b> du challenge Iris, mais nous avons rajoute quelques questions pour vous guider.\n",
    " <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'sample_code_submission/'      \n",
    "result_dir = 'sample_result_submission/' \n",
    "problem_dir = 'ingestion_program/'  \n",
    "score_dir = 'scoring_program/'\n",
    "from sys import path; path.append(model_dir); path.append(problem_dir); path.append(score_dir); \n",
    "%matplotlib inline\n",
    "# Comment the next lines to avoid auto-reload libraries if this causes problem with pickles in Python 3\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from checker import check\n",
    "import seaborn as sns; sns.set()\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h1> Step 1: Visualization </h1>\n",
    "<p>\n",
    "If you did not do the first TP, we strongly encourage you to do it. The first step of all data analyses is always to visualize data to understand it. Here is a subset of the visualizations you did last week.\n",
    "    <br>\n",
    "All challenge starting kits come with \"sample_data\": this usually a very small subset of the training data, split in the same way the entire dataset is split: a training set and two test sets (called \"valid\" and \"test\"). This allows you to test your code end-to-end.\n",
    "    <br>\n",
    "    Once you have tested everything, you can <b> substitute the \"sample_data\" with the \"public_data\" </b>: this is the entire dataset, except for the labels of the test sets.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'public_data'              # Change this to the directory where you put the public input data\n",
    "data_name = 'iris'\n",
    "#!ls $data_dir*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we load the data as a \"pandas\" data frame, so we can use \"pandas\" and \"seaborn\" built in functions to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_io import read_as_df\n",
    "data = read_as_df(data_dir  + '/' + data_name)                # The data are loaded as a Pandas Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataStats\n",
    "data.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataHist\n",
    "data.hist(figsize=(10, 10), bins=10, layout=(3, 2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ShowScatter\n",
    "sns.pairplot(data, hue=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Public data\n",
    "Apres avoir remplace data_dir par la directory \"public_data\", qu'obtenez-vous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = 4             # put correct value\n",
    "training_sample_number = 105   # put correct value\n",
    "validation_sample_number = 15  # put correct value\n",
    "test_sample_number = 30        # put correct value\n",
    "std_sepal_length = 0.795423    # Standard deviation of the sepal length, put correct value\n",
    "mean_sepal_width = 3.063810    # Mean of the sepal width, put correct value\n",
    "min_petal_length = 1.0    # Minimum value of the petal length, put correct value\n",
    "max_petal_width = 2.5     # Maximum value of the petal width, put correct value\n",
    "question = 1\n",
    "reponse = feature_number+training_sample_number+validation_sample_number+test_sample_number\n",
    "reponse += std_sepal_length+mean_sepal_width+min_petal_length+max_petal_width\n",
    "score = 0\n",
    "score += check(reponse, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h2>Loading data with DataManager</h2>\n",
    "    <p>\n",
    "We reload the data with the AutoML DataManager class because this will be more convenient for later use: we get a class that respects the AutoML format. <span style=\"color:red\">  One of the tasks of the <b> DATA VISUALIZATION BINOME </b> in your team will be to derive a class from DataManager to visualize data. </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_manager import DataManager\n",
    "D = DataManager(data_name, data_dir)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h2>Modifying the DataManager</h2>\n",
    "    <p>\n",
    "We provide in the directory \"my_code/\" sample code that you can modify, including a new DataManager object, derived from the original one. It contains a method that converts data to data frames, and, after that, allows you to call all sorts of methods to visualize data. You can keep expanding it using ideas from TP1 (heatmaps, correlation matrices, etc.)</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_code_dir = 'my_code/'       # Change the model to a better one once you have one!\n",
    "path.append(my_code_dir); \n",
    "from zDataManager import DataManager\n",
    "D = DataManager(data_name, data_dir)\n",
    "#print(D)\n",
    "set_name = 'test'             # Make sure your code works for \"train\", \"valid\", and \"test\"\n",
    "hh = D.DataHist(set_name)\n",
    "ss = D.ShowScatter(set_name)\n",
    "dd = D.DataStats(set_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Custom Data Manager\n",
    "Modifiez zDataManager (in my_code/) pour que les methodes DataHist, ShowScatter, et DataStats remplissent les fonctions desiree. Vous utiliserez les methodes des Pandas dataframes de la question precedente et vous retournerez l'objet correspondant.\n",
    "\n",
    "Idee 1: Ouvrez zDataManager.py dans un editeur.\n",
    "Idee 2: Utilisez \"Control F\" pour chercher DataHist, ShowScatter, et DataStats dans zDataManager et dans ce note book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 2\n",
    "reponse = len(hh.__class__.__name__+ss.__class__.__name__+dd.__class__.__name__)\n",
    "score += check(reponse, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "<h1>Step 2: Building a predictive model</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h2>Training a predictive model</h2>\n",
    "    <p>\n",
    "We provide in \"sample_code_submission/\" an example of predictive model (for classification or regression) in the `sample_code_submission/` directory. It is a quite stupid model: it makes constant predictions. Locate the file \"model.py\" and replace the code with that of the oneR model of TP1.\n",
    "        <br>\n",
    "        <span style=\"color:red\"> The role of the <b>PREDICTION BINOME</b> will be to create a good \"model.py\" </span>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_io import write\n",
    "from model import model\n",
    "# Uncomment the next line to show the code of the model\n",
    "#??model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "Create an instance of the model (run the constructor) and attempt to reload a previously saved version from `sample_code_submission/`:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = model()\n",
    "trained_model_name = model_dir + data_name\n",
    "# Uncomment the next line to re-load an already trained model\n",
    "#M = M.load(trained_model_name)                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Train the model (unless you reloaded a trained model) and make predictions. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = D.data['X_train']\n",
    "Y_train = D.data['Y_train']\n",
    "if not(M.is_trained):     \n",
    "    M.fit(X_train, Y_train)                     \n",
    "\n",
    "Y_hat_train = M.predict(D.data['X_train']) # Optional, not really needed to test on taining examples\n",
    "Y_hat_valid = M.predict(D.data['X_valid'])\n",
    "Y_hat_test = M.predict(D.data['X_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <b> Save the trained model </b> (will be ready to reload next time around) and save the prediction results. IMPORTANT: if you save the trained model, it will be bundled with your sample code submission. Therefore your model will NOT be retrained on the challenge platform. Remove the pickle from the submission if you want the model to be retrained on the platform.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.save(trained_model_name)                 \n",
    "result_name = result_dir + data_name\n",
    "from data_io import write\n",
    "write(result_name + '_train.predict', Y_hat_train)\n",
    "write(result_name + '_valid.predict', Y_hat_valid)\n",
    "write(result_name + '_test.predict', Y_hat_test)\n",
    "#!ls $result_name*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h2>Scoring the results</h2>\n",
    "    <h3>Load the challenge metric</h3>\n",
    "    <p>\n",
    "<b>The metric chosen for your challenge</b> is identified in the \"metric.txt\" file found in the `scoring_function/` directory. The function \"get_metric\" searches first for a metric having that name in my_metric.py, then in libscores.py, then in sklearn.metric.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libscores import get_metric\n",
    "metric_name, scoring_function = get_metric()\n",
    "print('Using scoring metric:', metric_name)\n",
    "# Uncomment the next line to display the code of the scoring metric\n",
    "#??scoring_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h3> Training performance </h3>\n",
    "    <p>\n",
    "The participants normally posess target values (labels) only for training examples (except for the sample data). We compute with the `example` metric the training score, which should be zero for perfect predictions.\n",
    "        </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_score = scoring_function(Y_train, Y_hat_train)\n",
    "print('Training score for the', metric_name, 'metric = %5.4f' % training_score)\n",
    "print('Ideal score for the', metric_name, 'metric = %5.4f' % scoring_function(Y_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h3>Cross-validation performance</h3>\n",
    "    <p>\n",
    "The participants do not have access to the labels Y_valid and Y_test to self-assess their validation and test performances. But training performance is not a good prediction of validation or test performance. Using cross-validation, the training data is split into multiple training/test folds, which allows participants to self-assess their model during development. The average CV result and 95% confidence interval is displayed.\n",
    "   </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(M, X_train, Y_train, cv=5, scoring=make_scorer(scoring_function))\n",
    "cv_score = scores.mean()\n",
    "cv_ebar = scores.std() * 2\n",
    "print('\\nCV score (95 perc. CI): %0.2f (+/- %0.2f)' % (cv_score, cv_ebar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Custom classifier\n",
    "Replacez la methode de classification de model.py dans sample_code_submission/ par le classifieur oneR du TP precedent. Refaites tourner l'apprentissage et l'evaluation ci-dessus. <br> <b> Attention: sauvegardez l'exemple original de model.py, on ne sais jamais...</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 3\n",
    "reponse = training_score+cv_score+cv_ebar\n",
    "score += check(reponse, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h2>Preprocessing and model selection</h2>\n",
    "    <p>\n",
    "You will probably try a lot of models and combinations of preprocessing and models during your project. \n",
    "        <br>\n",
    "        <span style=\"color:red\"> The role of the <b>PREPROCESSING BINOME</b> will be to experiment with various kinds of preprocessings: feature space reduction and/or augmentation.</span> The VISUALIZATION, PREPROCESSING, and PREDICTION binomes will have to collaborate to combine and compare methods.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Preprocessing\n",
    "We provide in the directory \"my_code/\" an example of preprocessing class called zPreprocessor. It performs Principal Component Analysis to reduce dimensions to two components.\n",
    "<br>\n",
    "In the cell below, copy the code snippet that transforms the instance D of the zDataManager object you created into a new instance obtained after PCA and apply the ShowScatter method to display the scatter plot of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zPreprocessor as zp\n",
    "\n",
    "Prepro = zp.Preprocessor()\n",
    " \n",
    "# Preprocess on the data and load it back into D\n",
    "D.data['X_train'] = Prepro.fit_transform(D.data['X_train'], D.data['Y_train'])\n",
    "D.data['X_valid'] = Prepro.transform(D.data['X_valid'])\n",
    "D.data['X_test'] = Prepro.transform(D.data['X_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 4\n",
    "reponse = D.ClfScatter(M, dim1=0, dim2=1, title='oneR')\n",
    "score += check(reponse, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est important de visualiser les resultats. Ici nous utilisons les scatter plots dans l'espace des deux premieres composantes principales. Pensez aussi a la <a href=\"http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\">matrice de confusion</a> et pour les problemes de classification binaire aux <a href=\"http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\">courbes ROC</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Model selection\n",
    "Vous allez devoir experimenter avec plusieurs modeles et combiner divers preprocessings. Une facon bien pratique de proceder est d'utiliser les <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\">Pipelines</a> de scikit-learn. D'autre part on a souvent de meilleures performances en faisant <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\">voter plusieurs classifieurs</a>.\n",
    "<br> Pour comprendre ce qui se passe, c'est important de visualiser les resultats. Modifiez le code de la fonction test() dans zClassifier.py qui se trouve dans my_code/ en remplaçant la fonction `compute_accuracy` par un appel de ClfScatter pour chaque classifieur que l'on teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zClassifier import test\n",
    "acc=test(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 5\n",
    "reponse = 2     # Indiquez le numero du classifieur qui overfitte le plus\n",
    "score += check(reponse+acc, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Your final score is %d / 5, congratulations!' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "<h1> Step 3: Making a submission </h1> \n",
    "\n",
    "<h2> Unit testing </h2> \n",
    "\n",
    "It is <b><span style=\"color:red\">important that you test your submission files before submitting them</span></b>. All you have to do to make a submission is modify the file <code>model.py</code> in the <code>sample_code_submission/</code> directory, then run this test to make sure everything works fine. This is the actual program that will be run on the server to test your submission. \n",
    "<br>\n",
    "Keep the sample code simple.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 $problem_dir/ingestion.py $data_dir $result_dir $problem_dir $model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "Also test the scoring program:\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_output_dir = 'scoring_output'\n",
    "!python3 $score_dir/score.py $data_dir $result_dir $scoring_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h1> Preparing the submission </h1>\n",
    "\n",
    "Zip the contents of `sample_code_submission/` (without the directory), or download the challenge public_data and run the command in the previous cell, after replacing sample_data by public_data.\n",
    "Then zip the contents of `sample_result_submission/` (without the directory).\n",
    "<b><span style=\"color:red\">Do NOT zip the data with your submissions</span></b>.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "from data_io import zipdir\n",
    "the_date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "sample_code_submission = '../sample_code_submission_' + the_date + '.zip'\n",
    "sample_result_submission = '../sample_result_submission_' + the_date + '.zip'\n",
    "zipdir(sample_code_submission, model_dir)\n",
    "zipdir(sample_result_submission, result_dir)\n",
    "print(\"Submit one of these files:\\n\" + sample_code_submission + \"\\n\" + sample_result_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
